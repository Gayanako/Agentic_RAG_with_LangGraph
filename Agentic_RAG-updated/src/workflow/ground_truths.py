GROUND_TRUTHS = {
    "what is rag evaluation?": 
        "RAG evaluation is the process of assessing how well a Retrieval-Augmented Generation (RAG) system retrieves relevant information and uses it to generate accurate, factually grounded responses",

    "what is rouge score?": 
        "A ROUGE score is a set of metrics used in natural language processing (NLP) to evaluate the quality of machine-generated text by comparing it to human references.",

    "what is the range of rouge score for a good model?":
        "A “good” ROUGE score depends on the task, but typically ROUGE-1, ROUGE-2, and ROUGE-L in the range of 0.4–0.6 indicate decent overlap, with 0.6+ considered strong. Scores vary with dataset and baseline."
}
